{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a tutorial for the data augmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the data augmentation by simply add a processor in the pipeline. Take Back-Translation for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forte.processors.data_augment import ReplacementDataAugmentProcessor\n",
    "from forte.pipeline import Pipeline\n",
    "from forte.data.multi_pack import MultiPack\n",
    "\n",
    "nlp = Pipeline[MultiPack]()\n",
    "\n",
    "# Configuration for the data augmentation processor.\n",
    "processor_config = {\n",
    "    'augment_entry': 'ft.onto.base_ontology.Token',\n",
    "    'other_entry_policy': {\n",
    "        'kwargs': {\n",
    "            'ft.onto.base_ontology.Document': 'auto_align',\n",
    "            'ft.onto.base_ontology.Sentence': 'auto_align'\n",
    "        }\n",
    "    },\n",
    "    'type': 'data_augmentation_op',\n",
    "    'data_aug_op': 'forte.processors.data_augment.algorithms.back_translation_op.BackTranslationOp',\n",
    "    'data_aug_op_config': {\n",
    "        'kwargs': {\n",
    "            'model_to': 'forte.processors.data_augment.algorithms.machine_translator.MarianMachineTranslator',\n",
    "            'model_back': 'forte.processors.data_augment.algorithms.machine_translator.MarianMachineTranslator',\n",
    "            'src_language': 'en',\n",
    "            'tgt_language': 'fr',\n",
    "            'device': 'cpu'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "processor = ReplacementDataAugmentProcessor()\n",
    "nlp.add(component=processor, config=processor_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example for typo data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forte.data.data_pack import DataPack\n",
    "from ft.onto.base_ontology import Token\n",
    "from forte.processors.data_augment.algorithms.typo_replacement_op import (\n",
    "    TypoReplacementOp,\n",
    ")\n",
    "\n",
    "opr = TypoReplacementOp(\n",
    "    configs={\n",
    "        \"prob\": 0.6,\n",
    "        'typo_generator': 'uniform',\n",
    "        'dict_path': 'https://raw.githubusercontent.com/wanglec/temporaryJson/main/misspelling.json'\n",
    "    }\n",
    ")\n",
    "data_pack = DataPack()\n",
    "data_pack.set_text(\"commonly addressable\")\n",
    "token_1 = Token(data_pack, 0, 8)\n",
    "token_2 = Token(data_pack, 9, 20)\n",
    "data_pack.add_entry(token_1)\n",
    "data_pack.add_entry(token_2)\n",
    "print(opr.replace(token_1))\n",
    "print(opr.replace(token_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Replacement Ops\n",
    "\n",
    "Lots of data augmentation methods can be considered as replacement-based approaches. That means, given a piece of input text, we will replace it with a new piece of augmented text. The back translation achieves this by translating the input into another language, then translating it back, and replacing the input with back-translated text.\n",
    "\n",
    "We wrapped these algorithms as the text replacement ops:\n",
    "\n",
    "1. *`DictionaryReplacementOp`*:\n",
    "    It utilizes the dictionaries, such as WORDNET, to replace the input word.\n",
    "2. *`BackTranslationOp`*:\n",
    "    It uses back translation to generate data with the same semantic meanings.\n",
    "3. *`DistributionReplacementOp`*:\n",
    "    It samples from a distribution to generate word-level new text.\n",
    "4. *`EmbeddingSimilarityOp`*:\n",
    "    It leverages pre-trained word embeddings, such as `word2vec` and `glove`, to replace the input word with another word with similar word embedding.\n",
    "\n",
    "The replacement ops should be under the *`'forte.processors.data_augment.algorithms'`*. To use these algorithms, set the value of *`'data_aug_op'`* in the configuration to full qualified name of the replacement op, and set the configuration of the replacement op in the *`'kwargs'`* under the field *`'data_aug_op_config'`*. Please check the documentation for specific configuration requirements of each replacement op."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacement-based Processor\n",
    "\n",
    "The processor *`'ReplacementDataAugmentProcessor'`* is responsible for managing the Forte data structures. Given a *`MultiPack`* as input, it will call the text replacement op to implement a specific data augmentation algorithm. Afterwards, it will handle the Forte data structures automatically. The output is the original *`MultiPack`*, with orignal *`DataPack`*s and new augmented *`DataPack`*s.\n",
    "\n",
    "For example, given an input *`MultiPack`*:\n",
    "\n",
    "    input(MultiPack){\n",
    "        dp1(DataPack): {\n",
    "            \"I love NLP!\"\n",
    "        }\n",
    "        dp2(DataPack): {\n",
    "            \"Forte makes NLP easier.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "The output should be the same *`MultiPack`* with new *`DataPack`*s:\n",
    "\n",
    "    output(MultiPack){\n",
    "        dp1(DataPack): {\n",
    "            \"I love NLP!\"\n",
    "        }\n",
    "        dp2(DataPack): {\n",
    "            \"Forte makes NLP easier.\"\n",
    "        }\n",
    "        dp1-aug(DataPack): {\n",
    "            \"I love Natural Language Processing!\"\n",
    "        }\n",
    "        dp2-aug(DataPack): {\n",
    "            \"Forte makes Natural Language Processing easier.\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "The *`'augment_entry'`* defines the entry the processor will augment. It should be a full qualified name of the entry class.\n",
    "\n",
    "The *`'other_entry_policy'`* specifies the policies for entries other than the *`'augment_entry'`*. If the policy is *`'auto_align'`*, the span of the Annotation will be automatically modified according to its original location.\n",
    "\n",
    "The *`Link`*, *`MultiPackLink`*, *`Group`* and *`MultiPackGroup`* are automatically copied if the *`Annotation`*s they attached to are present in the new *`DataPack`*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selector\n",
    "\n",
    "The data selector is used for pre-select data from the dataset that are suitable for data augmentation tasks. We support random and query-based elastic search data selectors that yield a subset of the original `Datapack`.\n",
    "\n",
    "There are two steps to perform data selection:\n",
    "1. Create an elastic search indexer from your data.\n",
    "2. Select data from the indexer according to the criteria: random or query-based.\n",
    "\n",
    "For details on how to create an indexer, and how to search for relevant documents, please refer to `examples/data_augmentation/data_select/README.md` for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning-based Data Augmentation model\n",
    "\n",
    "This model builds upon the connection of supervised learning and reinforcement learning (RL), and adapts reward learning algorithm from RL for joint data augmentation learning and model training. For details of the algorithm, please refer to the paper [Learning Data Manipulation for Augmentation and Weighting](https://arxiv.org/pdf/1910.12795.pdf).\n",
    "\n",
    "This algorithm updates data augmentation model parameters phi and downstream model parameters theta alternatingly. Two classes are designed to perform this algorithm: `forte/models/da_rl/MetaAugmentationWrapper` and `forte/models/da_rl/MetaModel`. \n",
    "\n",
    "`MetaAugmentationWrapper` wraps the data augmentation model, such as `BertForMaskedLM`, to perform functions needed in the algorithm that updates the augmentation model parameters phi. `MetaModel` is a `torch.nn.Module` that copies a model to update its parameters locally. It is a part of the algorithm to update parameters phi.\n",
    "\n",
    "To see how to use these two classes to build the RL-based DA model, and to see an example that uses this algorithm for text classification, please refer to `examples/data_augmentation/reinforcemennt/README.md` for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
