{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from termcolor import colored\n",
    "from texar.torch import HParams\n",
    "\n",
    "from nlp.pipeline import Pipeline\n",
    "from nlp.pipeline.data.ontology import conll03_ontology\n",
    "from nlp.pipeline.data.ontology.conll03_ontology import Token, Sentence, EntityMention, PredicateLink\n",
    "from nlp.pipeline.data.readers import StringReader\n",
    "from nlp.pipeline.processors.impl import (\n",
    "    NLTKWordTokenizer, NLTKSentenceSegmenter, NLTKPOSTagger, SRLPredictor, CoNLLNERPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates the pipeline here:\n",
    "\n",
    "## In a pipeline, processors should follow a consistent ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl = Pipeline(ontology=conll03_ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the reader of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.set_reader(StringReader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add processors\n",
    "## The processors can wrap any external tools. For example, we are wrapping some NLTK tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.add_processor(NLTKSentenceSegmenter())\n",
    "pl.add_processor(NLTKWordTokenizer())\n",
    "pl.add_processor(NLTKPOSTagger())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now load our own NER predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ner_configs = HParams(\n",
    "    {\n",
    "        'storage_path': os.path.join('../NER_model', 'resources.pkl')\n",
    "    },\n",
    "    CoNLLNERPredictor.default_hparams())\n",
    "\n",
    "pl.add_processor(CoNLLNERPredictor(), ner_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And here is our SRL predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "srl_configs = HParams(\n",
    "    {\n",
    "        'storage_path': '../SRL_model/',\n",
    "    },\n",
    "    SRLPredictor.default_hparams()\n",
    ")\n",
    "pl.add_processor(SRLPredictor(), srl_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.initialize_processors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our pipeline is ready, now let's try out some text snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_engine_text = \"A Scottish firm is looking to attract web surfers with a search engine that reads out results.\"\\\n",
    "                \" Called Speegle, it has the look and feel of a normal search engine, with the added feature of being able to read\"\\\n",
    "                \" out the results. Scottish speech technology firm CEC Systems launched the site in November. But experts have\"\\\n",
    "                \" questioned whether talking search engines are of any real benefit to people with visual impairments. The\"\\\n",
    "                \" Edinburgh-based firm CEC has married speech technology with ever-popular internet search. The ability to search is\"\\\n",
    "                \" becoming increasingly crucial to surfers baffled by the huge amount of information available on the web.\"\\\n",
    "\n",
    "win_medal_text = \"British hurdler Sarah Claxton is confident she can win her first major medal at next \"\\\n",
    "                \"month's European Indoor Championships in Madrid. Claxton will see if her new training \"\\\n",
    "                \"regime pays dividends at the European Indoors which take place on 5-6 March.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process this snippet with one simple command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shr/anaconda3/envs/pipeline_demo/lib/python3.7/site-packages/torch/nn/modules/rnn.py:525: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.num_layers, self.dropout, self.training, self.bidirectional)\n"
     ]
    }
   ],
   "source": [
    "pack = pl.process(win_medal_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now all the results are ready.\n",
    "## We have added the results as \"entries\" into our data.\n",
    "## Let's first take a look at the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSentence:\u001b[0m British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid. \n",
      "\n",
      "\u001b[31mSentence:\u001b[0m Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):  # returns an iterator of sentences in this pack\n",
    "    sent_text = sentence.text\n",
    "    print(colored(\"Sentence:\",'red'), sent_text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can access more fine-grained data in the sentences using our magical \"get\" function.\n",
    "## Let's get all the tokens in the first sentence and print out their Part-of-Speech value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTokens:\u001b[0m [('British', 'JJ'), ('hurdler', 'NN'), ('Sarah', 'NNP'), ('Claxton', 'NNP'), ('is', 'VBZ'), ('confident', 'JJ'), ('she', 'PRP'), ('can', 'MD'), ('win', 'VB'), ('her', 'PRP$'), ('first', 'JJ'), ('major', 'JJ'), ('medal', 'NN'), ('at', 'IN'), ('next', 'JJ'), ('month', 'NN'), (\"'s\", 'POS'), ('European', 'JJ'), ('Indoor', 'NNP'), ('Championships', 'NNP'), ('in', 'IN'), ('Madrid', 'NNP'), ('.', '.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    tokens = [(token.text, token.pos_tag) for token in\n",
    "              pack.get(Token, sentence)]  # get tokens in the span of \"sentence\"\n",
    "    print(colored(\"Tokens:\",'red'), tokens, \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarly, we can get all the named entities in the sentences, let's look at their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEntityMention:\u001b[0m British has type \u001b[34mMISC\u001b[0m \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Sarah Claxton has type \u001b[34mPER\u001b[0m \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m European Indoor Championships has type \u001b[34mMISC\u001b[0m \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Madrid has type \u001b[34mLOC\u001b[0m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    for entity in pack.get(EntityMention, sentence):\n",
    "        print(colored(\"EntityMention:\",'red'), \n",
    "              entity.text, \n",
    "              'has type', \n",
    "              colored(entity.ner_type, 'blue'), \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With this simple \"get\" function we can do a lot more. Let's see how one can play with semantic role labeling and NER at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSemantic role labels:\u001b[0m\n",
      "  - \"at next month's European Indoor Championships in Madrid\" is role AM-LOC of predicate \"win\"\n",
      "      Has entities: ['European Indoor Championships', 'Madrid'] \n",
      "\n",
      "  - \"can\" is role AM-MOD of predicate \"win\"\n",
      "      Has entities: [] \n",
      "\n",
      "  - \"she\" is role A0 of predicate \"win\"\n",
      "      Has entities: [] \n",
      "\n",
      "  - \"her first major medal\" is role A1 of predicate \"win\"\n",
      "      Has entities: [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    print(colored(\"Semantic role labels:\", 'red'))\n",
    "    # Here we can get all the links within this sentence.\n",
    "    for link in pack.get(PredicateLink, sentence):\n",
    "        parent = link.get_parent()\n",
    "        child = link.get_child()\n",
    "        print(f\"  - \\\"{child.text}\\\" is role {link.arg_type} of predicate \\\"{parent.text}\\\"\")\n",
    "        # get entities in the span of predicate args\n",
    "        entities = [entity.text for entity in pack.get(EntityMention, child)] \n",
    "        print(\"      Has entities:\", entities, \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEntityMention:\u001b[0m British\n",
      "    Has tokens: ['British'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Sarah Claxton\n",
      "    Has tokens: ['Sarah', 'Claxton'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m European Indoor Championships\n",
      "    Has tokens: ['European', 'Indoor', 'Championships'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Madrid\n",
      "    Has tokens: ['Madrid'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Claxton\n",
      "    Has tokens: ['Claxton'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m European Indoors\n",
      "    Has tokens: ['European', 'Indoors'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    for entity in pack.get(EntityMention, sentence):\n",
    "        print(colored(\"EntityMention:\",'red'), entity.text)\n",
    "        tokens = [token.text for token in pack.get(Token, entity)]\n",
    "        print(\"    Has tokens:\", tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
