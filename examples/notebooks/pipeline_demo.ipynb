{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from termcolor import colored\n",
    "from texar.torch import HParams\n",
    "\n",
    "from forte import Pipeline\n",
    "from forte.data.ontology import conll03_ontology\n",
    "from forte.data.ontology.conll03_ontology import Token, Sentence, EntityMention, PredicateLink\n",
    "from forte.data.readers import StringReader\n",
    "from forte.processors.impl import (\n",
    "    NLTKWordTokenizer, NLTKSentenceSegmenter, NLTKPOSTagger, SRLPredictor, CoNLLNERPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates the pipeline here:\n",
    "\n",
    "## In a pipeline, processors should follow a consistent ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl = Pipeline(ontology=conll03_ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the reader of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.set_reader(StringReader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add processors\n",
    "## The processors can wrap any external tools. For example, we are wrapping some NLTK tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.add_processor(NLTKSentenceSegmenter())\n",
    "pl.add_processor(NLTKWordTokenizer())\n",
    "pl.add_processor(NLTKPOSTagger())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now load our own NER predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ner_configs = HParams(\n",
    "    {\n",
    "        'storage_path': os.path.join('/home/hector/models/NER_model', 'resources.pkl')\n",
    "    },\n",
    "    CoNLLNERPredictor.default_hparams())\n",
    "\n",
    "pl.add_processor(CoNLLNERPredictor(), ner_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And here is our SRL predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "srl_configs = HParams(\n",
    "    {\n",
    "        'storage_path': '/home/hector/models/SRL_model/',\n",
    "    },\n",
    "    SRLPredictor.default_hparams()\n",
    ")\n",
    "pl.add_processor(SRLPredictor(), srl_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.initialize_processors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our pipeline is ready, now let's try out some text snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_engine_text = \"A Scottish firm is looking to attract web surfers with a search engine that reads out results.\"\\\n",
    "                \" Called Speegle, it has the look and feel of a normal search engine, with the added feature of being able to read\"\\\n",
    "                \" out the results. Scottish speech technology firm CEC Systems launched the site in November. But experts have\"\\\n",
    "                \" questioned whether talking search engines are of any real benefit to people with visual impairments. The\"\\\n",
    "                \" Edinburgh-based firm CEC has married speech technology with ever-popular internet search. The ability to search is\"\\\n",
    "                \" becoming increasingly crucial to surfers baffled by the huge amount of information available on the web.\"\\\n",
    "\n",
    "win_medal_text = \"British hurdler Sarah Claxton is confident she can win her first major medal at next \"\\\n",
    "                \"month's European Indoor Championships in Madrid. Claxton will see if her new training \"\\\n",
    "                \"regime pays dividends at the European Indoors which take place on 5-6 March.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process this snippet with one simple command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pack = pl.process(win_medal_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now all the results are ready.\n",
    "## We have added the results as \"entries\" into our data.\n",
    "## Let's first take a look at the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sentence in pack.get(Sentence):  # returns an iterator of sentences in this pack\n",
    "    sent_text = sentence.text\n",
    "    print(colored(\"Sentence:\",'red'), sent_text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can access more fine-grained data in the sentences using our magical \"get\" function.\n",
    "## Let's get all the tokens in the first sentence and print out their Part-of-Speech value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    tokens = [(token.text, token.pos_tag) for token in\n",
    "              pack.get(Token, sentence)]  # get tokens in the span of \"sentence\"\n",
    "    print(colored(\"Tokens:\",'red'), tokens, \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarly, we can get all the named entities in the sentences, let's look at their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    for entity in pack.get(EntityMention, sentence):\n",
    "        print(colored(\"EntityMention:\",'red'), \n",
    "              entity.text, \n",
    "              'has type', \n",
    "              colored(entity.ner_type, 'blue'), \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With this simple \"get\" function we can do a lot more. Let's see how one can play with semantic role labeling and NER at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    print(colored(\"Semantic role labels:\", 'red'))\n",
    "    # Here we can get all the links within this sentence.\n",
    "    for link in pack.get(PredicateLink, sentence):\n",
    "        parent = link.get_parent()\n",
    "        child = link.get_child()\n",
    "        print(f\"  - \\\"{child.text}\\\" is role {link.arg_type} of predicate \\\"{parent.text}\\\"\")\n",
    "        # get entities in the span of predicate args\n",
    "        entities = [entity.text for entity in pack.get(EntityMention, child)] \n",
    "        print(\"      Has entities:\", entities, \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    for entity in pack.get(EntityMention, sentence):\n",
    "        print(colored(\"EntityMention:\",'red'), entity.text)\n",
    "        tokens = [token.text for token in pack.get(Token, entity)]\n",
    "        print(\"    Has tokens:\", tokens, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
