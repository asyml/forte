{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "import pickle\n",
    "import os\n",
    "from nlp.pipeline.data.readers.conll03_reader import CoNLL03Ontology as Ont\n",
    "from nlp.pipeline.pipeline import Pipeline\n",
    "from nlp.pipeline.processors.impl.tokenization_predictor import NLTKWordTokenizer\n",
    "from nlp.pipeline.processors.impl.sentence_predictor import NLTKSentenceSegmenter\n",
    "from nlp.pipeline.processors.impl.postag_predictor import NLTKPOSTagger\n",
    "from nlp.pipeline.processors.impl.srl_predictor import SRLPredictor\n",
    "from nlp.pipeline.processors.impl.ner_predictor import CoNLLNERPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates the pipeline here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The pipeline can wrap any external tools, for example, we are wrapping some NLTK tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.processors.append(NLTKSentenceSegmenter())\n",
    "pl.processors.append(NLTKWordTokenizer())\n",
    "pl.processors.append(NLTKPOSTagger())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now load our own NER predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NER config and resources.\n",
    "ner_path = '/home/hector/models/NER_model/'\n",
    "ner_resource = pickle.load(open(os.path.join(ner_path, 'resources.pkl'), 'rb'))\n",
    "\n",
    "# Initialize the NER predictor.\n",
    "ner_predictor = CoNLLNERPredictor()\n",
    "ner_predictor.initialize(ner_resource)\n",
    "\n",
    "# Add it to the processors.\n",
    "pl.processors.append(ner_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And here is our SRL predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srl_predictor = SRLPredictor(model_dir=\"/home/hector/models/SRL_model/\")\n",
    "pl.processors.append(srl_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our pipeline is ready, now let's try out some text snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine_text = \"A Scottish firm is looking to attract web surfers with a search engine that reads out results.\"\\\n",
    "                \" Called Speegle, it has the look and feel of a normal search engine, with the added feature of being able to read\"\\\n",
    "                \" out the results. Scottish speech technology firm CEC Systems launched the site in November. But experts have\"\\\n",
    "                \" questioned whether talking search engines are of any real benefit to people with visual impairments. The\"\\\n",
    "                \" Edinburgh-based firm CEC has married speech technology with ever-popular internet search. The ability to search is\"\\\n",
    "                \" becoming increasingly crucial to surfers baffled by the huge amount of information available on the web.\"\\\n",
    "\n",
    "win_medal_text = \"British hurdler Sarah Claxton is confident she can win her first major medal at next \"\\\n",
    "                \"month's European Indoor Championships in Madrid.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process this snippet with one simple command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pack = pl.process(win_medal_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now all the results are ready.\n",
    "## We have added the results as \"entries\" into our data.\n",
    "## Let's take a look at the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in pack.get(Ont.Sentence):\n",
    "    sent_text = sentence.text\n",
    "    print(colored(\"Sentence:\",'red'), sent_text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can access more fine-grained data in the sentences using our magical \"get\" function.\n",
    "## Let's get all the tokens in the first sentence and print out their Part-of-Speech value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in pack.get(Ont.Sentence):\n",
    "    tokens = [(token.text, token.pos_tag) for token in\n",
    "              pack.get(Ont.Token, sentence)]\n",
    "    print(colored(\"Tokens:\",'red'), tokens, \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarly, we can get all the named entities in the sentences, let's look at their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sentence in pack.get(Ont.Sentence):\n",
    "    for entity in pack.get(Ont.EntityMention, sentence):\n",
    "        print(colored(\"EntityMention:\",'red'), \n",
    "              entity.text, \n",
    "              'has type', \n",
    "              colored(entity.ner_type, 'blue'), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With this simple \"get\" function we can do a lot more. Let's see how one can play with semantic role labeling and NER at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sentence in pack.get(Ont.Sentence):\n",
    "    print(colored(\"Semantic role labels:\", 'red'))\n",
    "    # Here we can get all the links within this sentence.\n",
    "    for link in pack.get(Ont.PredicateLink, sentence):\n",
    "        parent = link.get_parent()\n",
    "        child = link.get_child()\n",
    "        print(f\"  - \\\"{child.text}\\\" is role {link.arg_type} of predicate \\\"{parent.text}\\\"\")\n",
    "        entities = [entity.text for entity in pack.get(Ont.EntityMention, child)]\n",
    "        print(\"      Entities in predicate argument:\", entities, \"\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in pack.get(Ont.Sentence):\n",
    "    for entity in pack.get(Ont.EntityMention, sentence):\n",
    "        print(f\"Entity: {entity.text}\")\n",
    "        for token in pack.get(Ont.Token, entity):\n",
    "            print(f\"Has token {token.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}