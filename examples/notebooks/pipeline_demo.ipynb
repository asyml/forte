{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "\n",
    "from nlp.pipeline import Pipeline\n",
    "from nlp.pipeline.data.ontology import conll03_ontology\n",
    "from nlp.pipeline.data.ontology.conll03_ontology import Token, Sentence, EntityMention, PredicateLink\n",
    "from nlp.pipeline.data.readers import StringReader\n",
    "from nlp.pipeline.processors.impl import (\n",
    "    NLTKWordTokenizer, NLTKSentenceSegmenter, NLTKPOSTagger, SRLPredictor, CoNLLNERPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates the pipeline here:\n",
    "\n",
    "## In a pipeline, processors should follow a consistent ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl = Pipeline(ontology=conll03_ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the reader of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.set_reader(StringReader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add processors\n",
    "## The processors can wrap any external tools. For example, we are wrapping some NLTK tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.add_processor(NLTKSentenceSegmenter())\n",
    "pl.add_processor(NLTKWordTokenizer())\n",
    "pl.add_processor(NLTKPOSTagger())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now load our own NER predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the NER config and resources.\n",
    "ner_path = '../NER_model/'\n",
    "ner_resource = pickle.load(open(os.path.join(ner_path, 'resources.pkl'), 'rb'))\n",
    "\n",
    "# Initialize the NER predictor.\n",
    "ner_predictor = CoNLLNERPredictor()\n",
    "ner_predictor.initialize(ner_resource)\n",
    "\n",
    "# Add it to the processors.\n",
    "pl.add_processor(ner_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And here is our SRL predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "srl_predictor = SRLPredictor(model_dir=\"../SRL_model/\")\n",
    "pl.add_processor(srl_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our pipeline is ready, now let's try out some text snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_engine_text = \"A Scottish firm is looking to attract web surfers with a search engine that reads out results.\"\\\n",
    "                \" Called Speegle, it has the look and feel of a normal search engine, with the added feature of being able to read\"\\\n",
    "                \" out the results. Scottish speech technology firm CEC Systems launched the site in November. But experts have\"\\\n",
    "                \" questioned whether talking search engines are of any real benefit to people with visual impairments. The\"\\\n",
    "                \" Edinburgh-based firm CEC has married speech technology with ever-popular internet search. The ability to search is\"\\\n",
    "                \" becoming increasingly crucial to surfers baffled by the huge amount of information available on the web.\"\\\n",
    "\n",
    "win_medal_text = \"British hurdler Sarah Claxton is confident she can win her first major medal at next \"\\\n",
    "                \"month's European Indoor Championships in Madrid. Claxton will see if her new training \"\\\n",
    "                \"regime pays dividends at the European Indoors which take place on 5-6 March.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process this snippet with one simple command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pack = pl.process(win_medal_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now all the results are ready.\n",
    "## We have added the results as \"entries\" into our data.\n",
    "## Let's first take a look at the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSentence:\u001b[0m British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid. \n",
      "\n",
      "\u001b[31mSentence:\u001b[0m Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):  # returns an iterator of sentences in this pack\n",
    "    sent_text = sentence.text\n",
    "    print(colored(\"Sentence:\",'red'), sent_text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can access more fine-grained data in the sentences using our magical \"get\" function.\n",
    "## Let's get all the tokens in the first sentence and print out their Part-of-Speech value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTokens:\u001b[0m [('British', 'JJ'), ('hurdler', 'NN'), ('Sarah', 'NNP'), ('Claxton', 'NNP'), ('is', 'VBZ'), ('confident', 'JJ'), ('she', 'PRP'), ('can', 'MD'), ('win', 'VB'), ('her', 'PRP$'), ('first', 'JJ'), ('major', 'JJ'), ('medal', 'NN'), ('at', 'IN'), ('next', 'JJ'), ('month', 'NN'), (\"'s\", 'POS'), ('European', 'JJ'), ('Indoor', 'NNP'), ('Championships', 'NNP'), ('in', 'IN'), ('Madrid', 'NNP'), ('.', '.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    tokens = [(token.text, token.pos_tag) for token in\n",
    "              pack.get(Token, sentence)]  # get tokens in the span of \"sentence\"\n",
    "    print(colored(\"Tokens:\",'red'), tokens, \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarly, we can get all the named entities in the sentences, let's look at their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEntityMention:\u001b[0m British has type \u001b[34mMISC\u001b[0m \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Sarah Claxton has type \u001b[34mPER\u001b[0m \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m European Indoor Championships has type \u001b[34mMISC\u001b[0m \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Madrid has type \u001b[34mLOC\u001b[0m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    for entity in pack.get(EntityMention, sentence):\n",
    "        print(colored(\"EntityMention:\",'red'), \n",
    "              entity.text, \n",
    "              'has type', \n",
    "              colored(entity.ner_type, 'blue'), \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With this simple \"get\" function we can do a lot more. Let's see how one can play with semantic role labeling and NER at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSemantic role labels:\u001b[0m\n",
      "  - \"she\" is role A0 of predicate \"win\"\n",
      "      Has entities: [] \n",
      "\n",
      "  - \"at next month's European Indoor Championships in Madrid\" is role AM-LOC of predicate \"win\"\n",
      "      Has entities: ['European Indoor Championships', 'Madrid'] \n",
      "\n",
      "  - \"can\" is role AM-MOD of predicate \"win\"\n",
      "      Has entities: [] \n",
      "\n",
      "  - \"her first major medal\" is role A1 of predicate \"win\"\n",
      "      Has entities: [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    print(colored(\"Semantic role labels:\", 'red'))\n",
    "    # Here we can get all the links within this sentence.\n",
    "    for link in pack.get(PredicateLink, sentence):\n",
    "        parent = link.get_parent()\n",
    "        child = link.get_child()\n",
    "        print(f\"  - \\\"{child.text}\\\" is role {link.arg_type} of predicate \\\"{parent.text}\\\"\")\n",
    "        # get entities in the span of predicate args\n",
    "        entities = [entity.text for entity in pack.get(EntityMention, child)] \n",
    "        print(\"      Has entities:\", entities, \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mEntityMention:\u001b[0m British\n",
      "    Has tokens: ['British'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Sarah Claxton\n",
      "    Has tokens: ['Sarah', 'Claxton'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m European Indoor Championships\n",
      "    Has tokens: ['European', 'Indoor', 'Championships'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Madrid\n",
      "    Has tokens: ['Madrid'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m Claxton\n",
      "    Has tokens: ['Claxton'] \n",
      "\n",
      "\u001b[31mEntityMention:\u001b[0m European Indoors\n",
      "    Has tokens: ['European', 'Indoors'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in pack.get(Sentence):\n",
    "    for entity in pack.get(EntityMention, sentence):\n",
    "        print(colored(\"EntityMention:\",'red'), entity.text)\n",
    "        tokens = [token.text for token in pack.get(Token, entity)]\n",
    "        print(\"    Has tokens:\", tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}