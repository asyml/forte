{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "\n",
    "## Basics\n",
    "From input to output, we want a uniform module that manages the workflow step by step. For instance, given a data source in ``txt`` file for machine translation tasks, we want to read it from the file and use model to generate the translated text.\n",
    "\n",
    "Related Readings:\n",
    "\n",
    "* [Pipeline API](../code/pipeline.rst)\n",
    "\n",
    "## An working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up \n",
    "from forte.pipeline import Pipeline\n",
    "from forte.data.readers import PlainTextReader\n",
    "from forte.processors.third_party.machine_translation_processor import MachineTranslationProcessor\n",
    "import os\n",
    "\n",
    "\n",
    "# notebook should be running from project root folder\n",
    "dir_path = os.path.abspath(\n",
    "            os.path.join(\"data_samples\", \"machine_translation\")\n",
    "        )\n",
    "\n",
    "# pipeline code\n",
    "pipeline: Pipeline = Pipeline() # intialize a pipeline\n",
    "pipeline.set_reader(PlainTextReader())\n",
    "pipeline.add(MachineTranslationProcessor(), config={\n",
    "    \"pretrained_model\": \"t5-small\"\n",
    "})\n",
    "pipeline.run(dir_path) # it will call `initialize()` internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is derived from [machine translation task](wrap_MT_inference_pipeline.ipynb).\n",
    "\n",
    "After initializing `pipeline`, we need to read data from data source so we set reader first. Then we add `MachineTranslationProcessor` into `pipeline`. Finally, we run the pipeline and the output should be available under `mt_test_output` folder at the notebook directory. We can also pass `Dictionary` configuration while adding `PipelineComponent`.\n",
    "\n",
    "Normally, as we only need to read from data source once we set reader once. However, we can add multiple processors into pipeline when needed. For example, when data in one data pack is a paragraph, we might want to add `NLTKSentenceSegmentater` to segment paragraphs into sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus, readers and processors are all `PipelineComponent`. Let's consider the modules doing these tasks as `PipelineComponent`. Then, we focus on `Pipeline` which contains `PipelineComponent` and how it runs through the task.\n",
    "\n",
    "\n",
    "## Life Cycle\n",
    "Generally, there are four life cycles for pipeline.\n",
    "\n",
    "1. Before initializing: we add `PipelineComponent` into the pipelines.\n",
    "2. After initializing: we have all `PipelineComponent` in the pipeline initialized.\n",
    "3. Running pipeline.\n",
    "4. Finish pipeline: we collect resources used by the pipeline.\n",
    "\n",
    "\n",
    "\n",
    "## Pseudocode with PipelineComponent\n",
    "\n",
    "Let's check out a pseudocode for setting up and running a pipeline.\n",
    "\n",
    "```python\n",
    "pipeline: Pipeline = Pipeline() # intialize a pipeline\n",
    "pipeline.set_reader(SomePipelineComponent())\n",
    "pipeline.add(SomePipelineComponent())\n",
    "pipeline.add(SomePipelineComponent())\n",
    "pipeline.run(data_source) # it will call `initialize()` internally to initialize all :class:`PipelineComponent` in the pipeline.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we can see, after initializing a pipeline, we set `PipelineComponent` as reader which is the beginning of the workflow and add `PipelineComponent` into the workflow and then call `run()` on the data source. `PipelineComponent` keeps the order of adding internally, and it is same as the workflow order. As we can see the whole pipeline setup is easy and clean as it's a modular way of managing/running workflow.\n",
    "\n",
    "\n",
    "`PipelineComponent` can be [reader](../toc/reader.rst), [processor](../toc/processor.rst) or [selector](../toc/selector.rst). We will take a deeper look in the next sections.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef24553d1198fce4a0be9f455df40aff4e6272106653c30479d64479a9d4460b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('forte_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
