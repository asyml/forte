{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be774c44",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dade63",
   "metadata": {},
   "source": [
    "This is a very simple serialization demo that use the built-in JSON serializer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff03182",
   "metadata": {},
   "source": [
    "# Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5f7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/l/users/bhaskar.rao/forte_dev/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from forte.data.caster import MultiPackBoxer\n",
    "from forte.data.data_pack import DataPack\n",
    "from forte.data.multi_pack import MultiPack\n",
    "from forte.data.readers import OntonotesReader, DirPackReader\n",
    "from forte.data.readers.deserialize_reader import MultiPackDirectoryReader\n",
    "from forte.pipeline import Pipeline\n",
    "from forte.processors.base import MultiPackProcessor, MultiPackWriter\n",
    "from forte.processors.writers import PackNameJsonPackWriter\n",
    "from fortex.nltk import NLTKWordTokenizer, NLTKPOSTagger, NLTKSentenceSegmenter\n",
    "from ft.onto.base_ontology import EntityMention, CrossDocEntityRelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92cbd5f",
   "metadata": {},
   "source": [
    "# Functions and class definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f562686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackCopier(MultiPackProcessor):\n",
    "    \"\"\"\n",
    "    Copy the text from existing pack to a new pack.\n",
    "    \"\"\"\n",
    "\n",
    "    def _process(self, input_pack: MultiPack):\n",
    "        from_pack: DataPack = input_pack.get_pack(self.configs.copy_from)\n",
    "        copy_pack: DataPack = input_pack.add_pack(self.configs.copy_to)\n",
    "\n",
    "        copy_pack.set_text(from_pack.text)\n",
    "\n",
    "        if from_pack.pack_name is not None:\n",
    "            copy_pack.pack_name = from_pack.pack_name + \"_copy\"\n",
    "        else:\n",
    "            copy_pack.pack_name = \"copy\"\n",
    "\n",
    "        ent: EntityMention\n",
    "        for ent in from_pack.get(EntityMention):\n",
    "            EntityMention(copy_pack, ent.begin, ent.end)\n",
    "\n",
    "    @classmethod\n",
    "    def default_configs(cls) -> Dict[str, Any]:\n",
    "        return {\"copy_from\": \"default\", \"copy_to\": \"duplicate\"}\n",
    "\n",
    "\n",
    "class ExampleCoreferencer(MultiPackProcessor):\n",
    "    \"\"\"\n",
    "    Mark some example coreference relations.\n",
    "    \"\"\"\n",
    "\n",
    "    def _process(self, input_pack: MultiPack):\n",
    "        pack_i = input_pack.get_pack(\"default\")\n",
    "        pack_j = input_pack.get_pack(\"duplicate\")\n",
    "\n",
    "        for ent_i, ent_j in zip(\n",
    "            pack_i.get(EntityMention), pack_j.get(EntityMention)\n",
    "        ):\n",
    "            link = CrossDocEntityRelation(input_pack, ent_i, ent_j)\n",
    "            link.rel_type = \"coreference\"\n",
    "            input_pack.add_entry(link)\n",
    "\n",
    "\n",
    "class ExampleCorefCounter(MultiPackProcessor):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.coref_count = 0\n",
    "\n",
    "    def _process(self, input_pack: MultiPack):\n",
    "        rels = list(input_pack.get_entries_of(CrossDocEntityRelation))\n",
    "        self.coref_count += len(rels)\n",
    "\n",
    "    def finish(self, _):\n",
    "        print(f\"Found {self.coref_count} pairs in the multi packs.\")\n",
    "\n",
    "\n",
    "def pack_example(input_path, output_path):\n",
    "    \"\"\"\n",
    "    This example read data from input path and serialize to output path.\n",
    "    Args:\n",
    "        input_path:\n",
    "        output_path:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    print(\"Pack serialization example.\")\n",
    "    nlp = Pipeline[DataPack]()\n",
    "\n",
    "    nlp.set_reader(OntonotesReader())\n",
    "    nlp.add(NLTKSentenceSegmenter())\n",
    "    nlp.add(NLTKWordTokenizer())\n",
    "    nlp.add(NLTKPOSTagger())\n",
    "\n",
    "    # This is a simple writer that serialize the result to the current\n",
    "    # directory and will use the DocID field in the data pack as the file name.\n",
    "    nlp.add(\n",
    "        PackNameJsonPackWriter(),\n",
    "        {\n",
    "            \"output_dir\": output_path,\n",
    "            \"indent\": 2,\n",
    "            \"overwrite\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    nlp.run(input_path)\n",
    "\n",
    "\n",
    "def multi_example(input_path, output_path):\n",
    "    \"\"\"\n",
    "    This example reads data from input path, and write multi pack output\n",
    "    to output path.\n",
    "    Args:\n",
    "        input_path:\n",
    "        output_path:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    print(\"Multi Pack serialization example.\")\n",
    "\n",
    "    print(\n",
    "        \"We first read the data, and add multi-packs to them, and then \"\n",
    "        \"save the results.\"\n",
    "    )\n",
    "    coref_pl = Pipeline()\n",
    "    coref_pl.set_reader(DirPackReader())\n",
    "    coref_pl.add(MultiPackBoxer())\n",
    "    coref_pl.add(PackCopier())\n",
    "    coref_pl.add(ExampleCoreferencer())\n",
    "    coref_pl.add(ExampleCorefCounter())\n",
    "\n",
    "    coref_pl.add(\n",
    "        MultiPackWriter(),\n",
    "        config={\n",
    "            \"output_dir\": output_path,\n",
    "            \"indent\": 2,\n",
    "            \"overwrite\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    coref_pl.run(input_path)\n",
    "\n",
    "    print(\n",
    "        \"We can then load the saved results, and see if everything is OK. \"\n",
    "        \"We should see the same number of multi packs there. \"\n",
    "    )\n",
    "    reading_pl = Pipeline()\n",
    "    reading_pl.set_reader(\n",
    "        MultiPackDirectoryReader(),\n",
    "        config={\n",
    "            \"multi_pack_dir\": os.path.join(output_path, \"multi\"),\n",
    "            \"data_pack_dir\": os.path.join(output_path, \"packs\"),\n",
    "        },\n",
    "    )\n",
    "    reading_pl.add(ExampleCorefCounter())\n",
    "    reading_pl.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49011de3",
   "metadata": {},
   "source": [
    "# Dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a16b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='../../data_samples/ontonotes/00/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881408d",
   "metadata": {},
   "source": [
    "# serialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1828c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Re-declared a new class named [ConstituentNode], which is probably used in import.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pack serialization example.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/bhaskar.rao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/bhaskar.rao/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "WARNING:root:Re-declared a new class named [ConstituentNode], which is probably used in import.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi Pack serialization example.\n",
      "We first read the data, and add multi-packs to them, and then save the results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Re-declared a new class named [ConstituentNode], which is probably used in import.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 423 pairs in the multi packs.\n",
      "We can then load the saved results, and see if everything is OK. We should see the same number of multi packs there. \n",
      "Found 423 pairs in the multi packs.\n"
     ]
    }
   ],
   "source": [
    "pack_output = \"pack_out\"\n",
    "multipack_output = \"multi_out\"\n",
    "\n",
    "pack_example(data_path, pack_output)\n",
    "multi_example(pack_output, multipack_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forte_dev",
   "language": "python",
   "name": "forte_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
