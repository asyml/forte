{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98842a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/l/users/bhaskar.rao/forte_dev/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from forte.common.resources import Resources\n",
    "from forte.data.data_pack import DataPack\n",
    "from forte.datasets.wikipedia.dbpedia.db_utils import (\n",
    "    load_redirects,\n",
    "    print_progress,\n",
    ")\n",
    "from forte.datasets.wikipedia.dbpedia import (\n",
    "    DBpediaWikiReader,\n",
    "    WikiArticleWriter,\n",
    "    WikiStructReader,\n",
    "    WikiAnchorReader,\n",
    "    WikiPropertyReader,\n",
    "    WikiInfoBoxReader,\n",
    ")\n",
    "from forte.data.base_reader import PackReader\n",
    "from forte.datasets.wikipedia.dbpedia.dbpedia_datasets import (\n",
    "    WikiCategoryReader,\n",
    "    WikiPackReader,\n",
    ")\n",
    "from forte.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2ef206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_wiki_info(\n",
    "    reader: WikiPackReader,\n",
    "    resources: Resources,\n",
    "    wiki_info_data_path: str,\n",
    "    input_pack_path: str,\n",
    "    output_path: str,\n",
    "    prompt_name: str,\n",
    "    use_input_index=False,\n",
    "    skip_existing=True,\n",
    "    resume_from_last=False,\n",
    "    input_index_file_path: Optional[str] = \"article.idx\",\n",
    "    output_index_file_name: Optional[str] = \"article.idx\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Add wiki resource into the data pack.\n",
    "\n",
    "    Args:\n",
    "        reader: The info reader that loads the data pack.\n",
    "        resources: The resources object that should contain the redirects.\n",
    "        wiki_info_data_path: The path containing the wiki data.\n",
    "        input_pack_path: The initial data pack path.\n",
    "        output_path: The resulting output path.\n",
    "        prompt_name: a name to show during processing.\n",
    "        use_input_index: whether to use the input index to determine the\n",
    "          output path.\n",
    "        skip_existing: whether to skip this function if the folder exists.\n",
    "        resume_from_last: whether to resume from last end point, at most one\n",
    "          can be true between this and `skip_existing`\n",
    "        input_index_file_path: the full file path to the input index.\n",
    "        output_index_file_name: the file path to write the output index,\n",
    "            this is relative to `output_path`.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    pl = Pipeline[DataPack](resources)\n",
    "\n",
    "    if resume_from_last and skip_existing:\n",
    "        raise ValueError(\n",
    "            \"resume_from_last and skip_existing cannot both be \" \"true.\"\n",
    "        )\n",
    "\n",
    "    out_index_path = os.path.join(output_path, output_index_file_name)\n",
    "    if skip_existing and os.path.exists(out_index_path):\n",
    "        print_progress(\n",
    "            f\"\\n{out_index_path} exist, skipping {prompt_name}\", \"\\n\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    if resume_from_last:\n",
    "        if not os.path.exists(out_index_path):\n",
    "            raise ValueError(f\"Configured to do resume but path \"\n",
    "                             f\"{out_index_path} does not exists.\")\n",
    "\n",
    "        print_progress(\n",
    "            f\"\\nWill resume from last from {out_index_path}\", \"\\n\"\n",
    "        )\n",
    "        pl.set_reader(\n",
    "            reader,\n",
    "            config={\n",
    "                \"pack_index\": input_index_file_path,\n",
    "                \"pack_dir\": input_pack_path,\n",
    "                \"resume_index\": out_index_path,\n",
    "                \"zip_pack\": True,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        pl.set_reader(\n",
    "            reader,\n",
    "            config={\n",
    "                \"pack_index\": input_index_file_path,\n",
    "                \"pack_dir\": input_pack_path,\n",
    "                \"zip_pack\": True,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    pl.add(\n",
    "        WikiArticleWriter(),\n",
    "        config={\n",
    "            \"output_dir\": output_path,\n",
    "            \"zip_pack\": True,\n",
    "            \"drop_record\": True,\n",
    "            \"use_input_index\": use_input_index,\n",
    "            \"input_index_file\": input_index_file_path,\n",
    "            \"output_index_file\": output_index_file_name,\n",
    "            \"append_to_index\": resume_from_last,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print_progress(f\"Start running the {prompt_name} pipeline.\", \"\\n\")\n",
    "    pl.run(wiki_info_data_path)\n",
    "    print_progress(f\"Done collecting {prompt_name}.\", \"\\n\")\n",
    "\n",
    "\n",
    "def read_wiki_text(\n",
    "    nif_context: str,\n",
    "    output_dir: str,\n",
    "    resources: Resources,\n",
    "    skip_existing: bool = False,\n",
    "):\n",
    "    if skip_existing and os.path.exists(output_dir):\n",
    "        print_progress(f\"\\n{output_dir} exist, skipping reading text\", \"\\n\")\n",
    "        return\n",
    "\n",
    "    pl = Pipeline[DataPack](resources)\n",
    "    pl.set_reader(DBpediaWikiReader())\n",
    "    pl.add(\n",
    "        WikiArticleWriter(),\n",
    "        config={\n",
    "            \"output_dir\": output_dir,\n",
    "            \"zip_pack\": True,\n",
    "            \"drop_record\": True,\n",
    "        },\n",
    "    )\n",
    "    print_progress(\"Start running wiki text pipeline.\", \"\\n\")\n",
    "    pl.run(nif_context)\n",
    "    print_progress(\"Done collecting wiki text.\", \"\\n\")\n",
    "\n",
    "\n",
    "def cache_redirects(\n",
    "    base_output_path: str, redirect_path: str\n",
    ") -> Dict[str, str]:\n",
    "    redirect_pickle = os.path.join(base_output_path, \"redirects.pickle\")\n",
    "\n",
    "    redirect_map: Dict[str, str]\n",
    "    if os.path.exists(redirect_pickle):\n",
    "        redirect_map = pickle.load(open(redirect_pickle, \"rb\"))\n",
    "    else:\n",
    "        redirect_map = load_redirects(redirect_path)\n",
    "        with open(redirect_pickle, \"wb\") as pickle_f:\n",
    "            pickle.dump(redirect_map, pickle_f)\n",
    "    return redirect_map\n",
    "\n",
    "\n",
    "def main(\n",
    "    nif_context: str,\n",
    "    nif_page_structure: str,\n",
    "    mapping_literals: str,\n",
    "    mapping_objects: str,\n",
    "    nif_text_links: str,\n",
    "    redirects: str,\n",
    "    info_boxs_properties: str,\n",
    "    categories: str,\n",
    "    base_output_path: str,\n",
    "    resume_existing: bool,\n",
    "):\n",
    "    # Whether to skip the whole step.\n",
    "    if resume_existing:\n",
    "        skip_existing = False\n",
    "    else:\n",
    "        skip_existing = True\n",
    "\n",
    "    # The datasets are read in a few steps.\n",
    "    # 0. Load redirects between wikipedia pages.\n",
    "    print_progress(\"Loading redirects\", \"\\n\")\n",
    "\n",
    "    redirect_map: Dict[str, str] = cache_redirects(base_output_path, redirects)\n",
    "\n",
    "    resources: Resources = Resources()\n",
    "    resources.update(redirects=redirect_map)\n",
    "    print_progress(\"Done loading.\", \"\\n\")\n",
    "\n",
    "    # 1. Read the wiki text.\n",
    "    raw_pack_dir = os.path.join(base_output_path, \"nif_raw\")\n",
    "    read_wiki_text(nif_context, raw_pack_dir, resources, True)\n",
    "    print_progress(\"Done reading wikipedia text.\", \"\\n\")\n",
    "\n",
    "    # Use the same index structure for all writers.\n",
    "    main_index = os.path.join(raw_pack_dir, \"article.idx\")\n",
    "\n",
    "    # 2. Add wiki page structures, create a new directory for it.\n",
    "    struct_dir = raw_pack_dir + \"_struct\"\n",
    "    add_wiki_info(\n",
    "        WikiStructReader(),\n",
    "        resources,\n",
    "        nif_page_structure,\n",
    "        raw_pack_dir,\n",
    "        struct_dir,\n",
    "        \"page_structures\",\n",
    "        use_input_index=True,\n",
    "        skip_existing=skip_existing,\n",
    "        resume_from_last=resume_existing,\n",
    "        input_index_file_path=main_index,\n",
    "    )\n",
    "    print_progress(\"Done reading wikipedia structures.\", \"\\n\")\n",
    "\n",
    "    # 3. Add wiki links, create a new directory for it.\n",
    "    link_dir = struct_dir + \"_links\"\n",
    "    add_wiki_info(\n",
    "        WikiAnchorReader(),\n",
    "        resources,\n",
    "        nif_text_links,\n",
    "        struct_dir,\n",
    "        link_dir,\n",
    "        \"anchor_links\",\n",
    "        use_input_index=True,\n",
    "        skip_existing=True,\n",
    "        resume_from_last=resume_existing,\n",
    "        input_index_file_path=main_index,\n",
    "    )\n",
    "    print_progress(\"Done reading wikipedia anchors.\", \"\\n\")\n",
    "\n",
    "    # 4 The following steps add info boxes:\n",
    "    # 4.1 Add un-mapped infobox, we directly write to the previous directory\n",
    "    property_dir = link_dir\n",
    "    add_wiki_info(\n",
    "        WikiPropertyReader(),\n",
    "        resources,\n",
    "        info_boxs_properties,\n",
    "        link_dir,\n",
    "        property_dir,\n",
    "        \"info_box_properties\",\n",
    "        use_input_index=True,\n",
    "        skip_existing=True,\n",
    "        resume_from_last=resume_existing,\n",
    "        output_index_file_name=\"properties.idx\",\n",
    "        input_index_file_path=main_index,\n",
    "    )\n",
    "    print_progress(\"Done reading wikipedia info-boxes properties.\", \"\\n\")\n",
    "\n",
    "    # 4.1 Add mapped literal, we directly write to the previous directory.\n",
    "    literal_dir = property_dir\n",
    "    add_wiki_info(\n",
    "        WikiInfoBoxReader(),\n",
    "        resources,\n",
    "        mapping_literals,\n",
    "        property_dir,\n",
    "        literal_dir,\n",
    "        \"literals\",\n",
    "        use_input_index=True,\n",
    "        skip_existing=True,\n",
    "        resume_from_last=resume_existing,\n",
    "        output_index_file_name=\"literals.idx\",\n",
    "        input_index_file_path=main_index,\n",
    "    )\n",
    "    print_progress(\"Done reading wikipedia info-boxes literals.\", \"\\n\")\n",
    "\n",
    "    # 4.1 Add mapped object, we directly write to the previous directory.\n",
    "    mapping_dir = literal_dir\n",
    "    add_wiki_info(\n",
    "        WikiInfoBoxReader(),\n",
    "        resources,\n",
    "        mapping_objects,\n",
    "        literal_dir,\n",
    "        mapping_dir,\n",
    "        \"objects\",\n",
    "        use_input_index=True,\n",
    "        skip_existing=True,\n",
    "        resume_from_last=resume_existing,\n",
    "        output_index_file_name=\"objects.idx\",\n",
    "        input_index_file_path=main_index,\n",
    "    )\n",
    "    print_progress(\"Done reading wikipedia info-boxes objects.\", \"\\n\")\n",
    "\n",
    "    # 4.2 Add category, directly write to previous directory.\n",
    "    category_dir = mapping_dir\n",
    "    add_wiki_info(\n",
    "        WikiCategoryReader(),\n",
    "        resources,\n",
    "        categories,\n",
    "        mapping_dir,\n",
    "        category_dir,\n",
    "        \"categories\",\n",
    "        use_input_index=True,\n",
    "        skip_existing=True,\n",
    "        resume_from_last=resume_existing,\n",
    "        output_index_file_name=\"categories.idx\",\n",
    "        input_index_file_path=main_index,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_path(dataset: str):\n",
    "    p = os.path.join(base_dir, dataset)\n",
    "    if os.path.exists(p):\n",
    "        return p\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"The dataset {dataset} is not found in \"\n",
    "            f\"base directory {base_dir}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868b4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "base_dir = '../../data_samples/dbpedia'\n",
    "pack_output = 'sample_output'\n",
    "samples=True\n",
    "l=[base_dir,pack_output,samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aded7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_samples = \"TRUE\"\n",
    "will_resume = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce4461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(pack_output):\n",
    "    os.makedirs(pack_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be55910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    filename=os.path.join(pack_output, \"dump.log\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d9348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c58485",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_samples = False\n",
    "if len(l) > 3:\n",
    "    with_samples = l[1]\n",
    "    try_samples = with_samples.upper().startswith(\"TRUE\")\n",
    "\n",
    "will_resume = False\n",
    "if len(l) > 4:\n",
    "    resume = l[4]\n",
    "    will_resume = resume.upper().startswith(\"TRUE\")\n",
    "\n",
    "if not os.path.exists(pack_output):\n",
    "    os.makedirs(pack_output)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    filename=os.path.join(pack_output, \"dump.log\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b76d913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K -- Loading redirects\n",
      "\u001b[K -- Done loading.\n",
      "\u001b[K -- \n",
      "sample_output/nif_raw exist, skipping reading text\n",
      "\u001b[K -- Done reading wikipedia text.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Configured to do resume but path sample_output/nif_raw_struct/article.idx does not exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m try_samples:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnif_context.tql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnif_page_structure.tql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliterals.tql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmappingbased_objects_en.tql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_links.tql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mredirects.tql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfobox_properties_mapped_en.tql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marticle_categories_en.tql\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpack_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwill_resume\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     main(\n\u001b[1;32m     16\u001b[0m         get_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnif_context_en.tql.bz2\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     17\u001b[0m         get_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnif_page_structure_en.tql.bz2\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m         will_resume,\n\u001b[1;32m     26\u001b[0m     )\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(nif_context, nif_page_structure, mapping_literals, mapping_objects, nif_text_links, redirects, info_boxs_properties, categories, base_output_path, resume_existing)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# 2. Add wiki page structures, create a new directory for it.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m struct_dir \u001b[38;5;241m=\u001b[39m raw_pack_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_struct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 173\u001b[0m \u001b[43madd_wiki_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mWikiStructReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnif_page_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_pack_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstruct_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpage_structures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_input_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_existing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_index_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m print_progress(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone reading wikipedia structures.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# 3. Add wiki links, create a new directory for it.\u001b[39;00m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36madd_wiki_info\u001b[0;34m(reader, resources, wiki_info_data_path, input_pack_path, output_path, prompt_name, use_input_index, skip_existing, resume_from_last, input_index_file_path, output_index_file_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_from_last:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(out_index_path):\n\u001b[0;32m---> 52\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfigured to do resume but path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_index_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     print_progress(\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWill resume from last from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_index_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m     pl\u001b[38;5;241m.\u001b[39mset_reader(\n\u001b[1;32m     59\u001b[0m         reader,\n\u001b[1;32m     60\u001b[0m         config\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m         },\n\u001b[1;32m     66\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Configured to do resume but path sample_output/nif_raw_struct/article.idx does not exists."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "if try_samples:\n",
    "    main(\n",
    "        get_path(\"nif_context.tql\"),\n",
    "        get_path(\"nif_page_structure.tql\"),\n",
    "        get_path(\"literals.tql\"),\n",
    "        get_path(\"mappingbased_objects_en.tql\"),\n",
    "        get_path(\"text_links.tql\"),\n",
    "        get_path(\"redirects.tql\"),\n",
    "        get_path(\"infobox_properties_mapped_en.tql\"),\n",
    "        get_path(\"article_categories_en.tql\"),\n",
    "        pack_output,\n",
    "        will_resume,\n",
    "    )\n",
    "else:\n",
    "    main(\n",
    "        get_path(\"nif_context_en.tql.bz2\"),\n",
    "        get_path(\"nif_page_structure_en.tql.bz2\"),\n",
    "        get_path(\"mappingbased_literals_en.tql.bz2\"),\n",
    "        get_path(\"mappingbased_objects_en.tql.bz2\"),\n",
    "        get_path(\"nif_text_links_en.tql.bz2\"),\n",
    "        get_path(\"redirects_en.tql.bz2\"),\n",
    "        get_path(\"infobox_properties_mapped_en.tql.bz2\"),\n",
    "        get_path(\"article_categories_en.tql.bz2\"),\n",
    "        pack_output,\n",
    "        will_resume,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4509d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff783a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forte_dev",
   "language": "python",
   "name": "forte_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
