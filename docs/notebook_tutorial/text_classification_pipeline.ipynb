{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from termcolor import colored\n",
    "from forte.data.readers import ClassificationDatasetReader\n",
    "from fortex.huggingface import ZeroShotClassifier\n",
    "from forte.pipeline import Pipeline\n",
    "from fortex.nltk import NLTKSentenceSegmenter\n",
    "from ft.onto.base_ontology import Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "This notebook tutorial is derived from https://github.com/asyml/forte/tree/master/examples/classification\n",
    "Given a table-like csv file with data at some columns are input text and data at one column is label, we set up a text classification pipeline below. This example is also a good example of wrapping external library classes/methods into `PipelineComponent`.\n",
    "\n",
    "\n",
    "## Inference Workflow\n",
    "\n",
    "### Pipeline\n",
    "User can refer to the code link [here](https://github.com/asyml/forte/blob/master/examples/classification/bank_customer_intent.py#L123): \n",
    "\n",
    "The pipeline has one reader `ClassificationDatasetReader` and two processor\n",
    "`NLTKSentenceSegmenter` and `ZeroShotClassifier`. \n",
    "\n",
    "\n",
    "### Reader\n",
    "User can refer to the code [here](https://github.com/asyml/forte/blob/7dc6e6c7d62d9a4126bdfc5ca02d15be3ffd61ca/forte/data/readers/classification_reader.py#L26): \n",
    "\n",
    "\n",
    "* `set_up()`: It checks whether the configuration is correct. For example, `skip_k_starting_lines` should be larger than 0 otherwise it doesn't make sense. It also converts different table data at the label column to a digit.\n",
    "* `initialize()`: intialize resources? User can register\n",
    "                shareable resources here, for example, the vocabulary.\n",
    "* `_collect()`: read rows from csv file and returns iterator that yields line id and line data.\n",
    "* `_cache_key_function()`: use the line id as the cache key. \n",
    "* `_parse_pack()`: parse data from iterator returned by `_collect` and load it in the datapack\n",
    "\n",
    "\n",
    "\n",
    "### Processor\n",
    "\n",
    "[NLTKSentenceSegmenter](https://github.com/asyml/forte-wrappers/blob/80cfe19926c0596edd13985581e8ca01a7be86ad/src/nltk/fortex/nltk/nltk_processors.py#L247)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Huggingface classifier\n",
    "https://github.com/asyml/forte-wrappers/blob/main/src/huggingface/fortex/huggingface/zero_shot_classifier.py\n",
    "\n",
    "\n",
    "\n",
    "RequestPackingProcessor\n",
    "\n",
    "A processor that implements the packing batch processor, using a\n",
    "    variation of the fixed size batcher\n",
    "    :class:`~forte.data.batchers.FixedSizeRequestDataPackBatcher`,\n",
    "    which will use `DataPack.get_data` function with the`context_type`\n",
    "    and `requests` parameters.\n",
    "\n",
    "class PackingBatchProcessor(BaseBatchProcessor[PackType], ABC):\n",
    "    \"\"\"\n",
    "    This class extends the BaseBatchProcessor class and provide additional\n",
    "    utilities to align and pack the extracted results back to the data pack.\n",
    "\n",
    "    To implement this processor, one need to implement:\n",
    "    1. The `predict` function that make predictions for each input data batch.\n",
    "    2. The `pack` function that add the prediction value back to the data pack.\n",
    "\n",
    "    Users that implement the processor only have to concern about a single\n",
    "    batch, the alignment between the data batch and the data pack will be\n",
    "    maintained by the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "csv_path = \"data_samples/amazon_review_polarity_csv/sample.csv\"\n",
    "pl = Pipeline()\n",
    "\n",
    "# initialize labels\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "index2class = dict(enumerate(class_names))\n",
    "pl.set_reader(\n",
    "    ClassificationDatasetReader(), config={\"index2class\": index2class}\n",
    ")\n",
    "pl.add(NLTKSentenceSegmenter())\n",
    "pl.add(ZeroShotClassifier(), config={\"candidate_labels\": class_names})\n",
    "pl.initialize()\n",
    "\n",
    "\n",
    "for pack in pl.process_dataset(csv_path):\n",
    "    for sent in pack.get(Sentence):\n",
    "        if (\n",
    "            input(\"Type n for the next documentation and its prediction: \").lower()\n",
    "            == \"n\"\n",
    "        ):\n",
    "            sent_text = sent.text\n",
    "            print(colored(\"Sentence:\", \"red\"), sent_text, \"\\n\")\n",
    "            print(colored(\"Prediction:\", \"blue\"), sent.classification)\n",
    "        else:\n",
    "            print(\"Exit the program due to unrecognized input\")\n",
    "            sys.exit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
