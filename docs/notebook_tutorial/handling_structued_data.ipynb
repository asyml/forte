{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Structured Data in DataPack\n",
    "\n",
    "## Related Readings\n",
    "- [DataPack API](../code/data.rst#DataPack)\n",
    "\n",
    "\n",
    "## Retrieve data\n",
    "`DataPack.get()` and `DataPack.get_data()` are methods commonly used to retrieve data from a `DataPack`.\n",
    "Let's start with introducing `DataPack.get()` which returns a generator that generate requested data __instance__.\n",
    "\n",
    "We can set up the `data_pack` using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Re-declared a new class named [ConstituentNode], which is probably used in import.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from forte.data.data_pack import DataPack\n",
    "from forte.pipeline import Pipeline\n",
    "from forte.utils import utils\n",
    "from ft.onto.base_ontology import (\n",
    "    Token,\n",
    "    Sentence,\n",
    "    Document,\n",
    "    AudioAnnotation,\n",
    "    AudioUtterance,\n",
    ")\n",
    "from forte.data.ontology import Annotation\n",
    "from forte.data.readers import OntonotesReader, AudioReader\n",
    "from forte.data.data_pack import DataPack\n",
    "from forte.pipeline import Pipeline\n",
    "data_path = os.path.abspath(\n",
    "                os.path.join(\"../../data_samples\", \"ontonotes/one_file\")\n",
    "            )\n",
    "pipeline: Pipeline = Pipeline()\n",
    "pipeline.set_reader(OntonotesReader())\n",
    "pipeline.initialize()\n",
    "data_pack: DataPack = pipeline.process_one(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code explains how to retrieve data instance and access data fields in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 document instance:   Document(document_class=[], sentiment={}, classifications=<forte.data.ontology.core.FDict object at 0x7effc005a8d0>)\n",
      "0 document text:   The Indonesian billionaire James Riady has agreed to pay $ 8.5 million and plead guilty to illegally donating money for Bill Clinton 's 1992 presidential campaign . He admits he was trying to influence American policy on China .\n"
     ]
    }
   ],
   "source": [
    "for doc_idx, instance in enumerate(data_pack.get(Document)):\n",
    "    print(doc_idx, \"document instance:  \", instance)\n",
    "    print(doc_idx, \"document text:  \", instance.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can get data instance from the generator returned by `data_pack.get(Document)`. And we can get the document text by `instance.text`.\n",
    "\n",
    "By contrast, `DataPack.get_data()` returns a generator that generates __dictionaries__ containing data requested, and each dictionary has a scope that covers __certain range of data__  in the `DataPack`.\n",
    "\n",
    "To understand this, let's consider a dummy case.\n",
    "Given that there is a document in the `DataPack` instance `data_pack`, we want to get the full document in `data_pack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we can run the following code to get the full document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :   The Indonesian billionaire James Riady has agreed to pay $ 8.5 million and plead guilty to illegally donating money for Bill Clinton 's 1992 presidential campaign . He admits he was trying to influence American policy on China .\n"
     ]
    }
   ],
   "source": [
    "for doc_idx, doc_d in enumerate(data_pack.get_data(context_type=Document)):\n",
    "    print(doc_idx, \":  \", doc_d['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the generator generates a dictionary each iteration (in this dummy case we only have one iteration) and the document data is retrieved by dictionary key `'context'`.\n",
    "\n",
    "To better understand this, let's consider a more concrete case. Since the document containing two sentences, suppose we want to retrieve text data sentence by sentence for a linguistic analysis task. In other words, we expect two dictionaries in the generator and each dictionary stores a sentence.\n",
    "\n",
    "We can get each sentence by the following code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The Indonesian billionaire James Riady has agreed to pay $ 8.5 million and plead guilty to illegally donating money for Bill Clinton 's 1992 presidential campaign .\n",
      "1 He admits he was trying to influence American policy on China .\n"
     ]
    }
   ],
   "source": [
    "data_generator = data_pack.get_data(context_type=Sentence)\n",
    "for sent_idx, sent_d in enumerate(data_generator):\n",
    "    print(sent_idx, sent_d['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get the two sentences by two iterations.\n",
    "\n",
    "So far, we introduce two examples to explain the first parameter `context_type` which controls the granularity of the data context. Depending on the task, we can generate data of different granularities. We assigned `context_type` from `Document` to `Sentence` for sentence tasks, and we can even further change it to `Token` for token tasks.\n",
    "\n",
    "Suppose we don't want to analyze the first sentence in the `data_pack`, there is `skip_k` parameter that skips k data of `context_type` and starts generating data from (k+1)th instance. In this case, we want to start generating from the second instance so we set `skip_k` to 1 to skip the first instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 He admits he was trying to influence American policy on China .\n"
     ]
    }
   ],
   "source": [
    "data_generator = data_pack.get_data(context_type=Sentence, skip_k=1)\n",
    "for sent_idx, sent_d in enumerate(data_generator):\n",
    "    print(sent_idx, sent_d['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we have introduced three \"data types\", `Document`, `Sentence`, and `Token`. They are three common data entries for text analysis. \n",
    "\n",
    "They are also subclasses of `Annotation` which is a parent class for text data entries and can record text span which is the range of data that we have been explaining. However, such a retrieval is usually not flexible enough for a real task. \n",
    "\n",
    "Suppose we want to do part-of-speech tagging for each sentence, it means we need to tag `Token` pos within each sentence. Therefore, we need data entries of `Token` and `Sentence`. Moreover, we want to analyze POS sentence by sentence and `Token` data entries and its POS are better nested in retrieved `Sentence` data. Same as before, we should set `context_type` to be `Sentence`. Moreover, we introduce parameter `request` which supports retrieval of `Token` and its POS within the scope of `Sentence` context type.\n",
    "\n",
    "See the example below for how to set `requests`, and for simplicity we still skip the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 He admits he was trying to influence American policy on China .\n",
      "['PRP' 'VBZ' 'PRP' 'VBD' 'VBG' 'TO' 'VB' 'JJ' 'NN' 'IN' 'NNP' '.']\n",
      "Token list length: 12\n",
      "POS list length: 12\n"
     ]
    }
   ],
   "source": [
    "requests = {\n",
    "    Token: [\"pos\"],\n",
    "}\n",
    "data_generator = data_pack.get_data(context_type=Sentence, request=requests, skip_k=1)\n",
    "for sent_idx, sent_d in enumerate(data_generator):\n",
    "    print(sent_idx, sent_d['context'])\n",
    "    print(sent_d['Token']['pos'])\n",
    "    print(\"Token list length:\", len(sent_d['Token'][\"text\"]))\n",
    "    print(\"POS list length:\", len(sent_d['Token']['pos']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example, we can see `requests` is dictionary where keys are data entries of `Annotation` type and values are requested data entry attributes. And the retrieved data dictionary `sent_d` now has key `Token`, and `sent_d['Token']` is a dictionary has a key `pos`. It's exactly data entries what we requested.\n",
    "\n",
    "\n",
    "Moreover, we should pay attention to the range of `Token` data, values of `sent_d['Token']` is a list of data which are all within one sentence, and lists' length are all the same since each list item is one `Token`'s data.\n",
    "\n",
    "\n",
    "See example below to see the dissembled data and their correspondence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 He admits he was trying to influence American policy on China .\n",
      "He PRP\n",
      "admits VBZ\n",
      "he PRP\n",
      "was VBD\n",
      "trying VBG\n",
      "to TO\n",
      "influence VB\n",
      "American JJ\n",
      "policy NN\n",
      "on IN\n",
      "China NNP\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "data_generator = data_pack.get_data(context_type=Sentence, request=requests, skip_k=1)\n",
    "for sent_idx, sent_d in enumerate(data_generator):\n",
    "    print(sent_idx, sent_d['context'])\n",
    "    for token_txt, token_pos in (zip(sent_d['Token']['text'], sent_d['Token']['pos'])):\n",
    "        print(token_txt, token_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['context', 'offset', 'tid'])\n",
      "dict_keys(['context', 'offset', 'tid', 'Token'])\n",
      "dict_keys(['context', 'offset', 'tid'])\n"
     ]
    }
   ],
   "source": [
    "# intialize a token data dictionary\n",
    "data_generator = data_pack.get_data(context_type=Token, skip_k=1)\n",
    "token_d = next(data_generator)\n",
    "\n",
    "print(doc_d.keys()) # document data dictionary\n",
    "print(sent_d.keys()) # sentence data dictionary\n",
    "print(token_d.keys()) # token data dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we check dictionary keys for document, sentence and token data returned by the `get_data` method, there are four data fields. Except for `Token` we requested earlier, all other three are returned by default. \n",
    "\n",
    "A natural question arises: do those data classes have a parent class that shares common attributes of `'context', 'offset', 'tid'`. The answer is positive. We have `Annotation` class that represent generic text data.\n",
    "* `context`: data within the context type scope.\n",
    "* `offset`: the first character of the text class index\n",
    "* `tid`: id of the text data instances.\n",
    "\n",
    "Below we will dive in the attributes of `Annotation` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Annotation\n",
    "\n",
    "In forte, each annotation has an attribute `span` which represents begin and end of annotation-specific data of that particular annotation. For `Annotation` type, range means the begin index and end index of characters under `Annotation` type in the `text` payload of the `DataPack`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " For an `Token` instance which is a subtype of `Annotation`, its annotation-specific data is `text` and therefore range means the begin and end of characters of that `Token` instance. For an `Recording` instance which is a subtype of `AudioAnnotation`, its annotation-specific data is `audio` and there range means the begin and end index of that `Recording` instance.\n",
    "\n",
    "\n",
    "\n",
    "As we are extending forte's capabilities of dealing more modalities, we also have a parent class for audio data which is `AudioAnnotation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioAnnotation\n",
    "Based on the idea of \"range\", in the example code, entry `AudioUtterance` will be searched in `DataPack.audio_annotations` and the requested data field `speaker` will be included in the generator's data.\n",
    "\n",
    "For `AudioAnnotation` type, range means the begin index and end index of sound sample under `AudioAnnotation` type in the `audio` payload of the `DataPack`. \n",
    "\n",
    "For example, if User wants to get data of `AudioAnnotation` from a `DataPack` instance `pack`. User can call the function like the code blow. It returns a generator that User can iterate over.\n",
    "`AudioAnnotation` is passed into the method as parameter `context_type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Coverage Index\n",
    "`DataPack.get()` is commonly used to retrieve entries from a datapack. In some cases, we are only interested in getting entries from a specific range. `DataPack.get()` allows users to set `range_annotation` which controls the search area of the sub-types. If `DataPack.get()` is called frequently with queries related to the `range_annotation`, you may consider building the coverage index regarding the related entry types. Users can call `DataPack.build_coverage_for(context_type, covered_type)` in order to create a mapping between a pair of entry types and target entries that are covered in ranges specified by outer entries.\n",
    "\n",
    "For example, if you need to get all the `Token`s from some `Sentence`, you can write your code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the sentences in the pack.\n",
    "for sentence in data_pack.get(Sentence):\n",
    "    # Take all tokens from a sentence\n",
    "    token_entries = data_pack.get(\n",
    "        entry_type=Token, range_annotation=sentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the snippet above may become a bottleneck if you have a lot of `Sentence` and `Token` entries inside the datapack. To speed up this process, you can build a coverage index first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build coverage index between `Token` and `Sentence`\n",
    "data_pack.build_coverage_for(\n",
    "    context_type=Sentence,\n",
    "    covered_type=Token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `DataPack.build_coverage_for(context_type, covered_type)` function is able to build a mapping from `context_type` to `covered_type`, allowing faster retrieval of inner entries covered by outer entries inside the datapack.\n",
    "We also provide a function called `DataPack.covers(context_entry, covered_entry)` for coverage checking. It returns `True` if the span of `covered_entry` is covered by the span of `context_entry`.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35c5ad93f26e4a012fe0ce2a15a836f7204b7396c27ea7588e034222fd2bde38"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('forte_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
