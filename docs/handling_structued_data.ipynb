{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Structured Data\n",
    "\n",
    "\n",
    "## Retrieve data\n",
    "`DataPack.get_data()` is commonly used to retrieve data from a `DataPack`. This method returns a generator that generates dictionaries containing data requested, and each dictionary has a scope that covers __certain range of data__  in the `DataPack`.\n",
    "To understand this, let's consider a dummy case.  Given that there is a document in the `DataPack` instance `data_pack`, we want to get the full document in `data_pack`.\n",
    "We can set up the `data_pack` using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Re-declared a new class named [ConstituentNode], which is probably used in import.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from forte.data.data_pack import DataPack\n",
    "from forte.pipeline import Pipeline\n",
    "from forte.utils import utils\n",
    "from ft.onto.base_ontology import (\n",
    "    Token,\n",
    "    Sentence,\n",
    "    Document,\n",
    "    PredicateArgument,\n",
    "    PredicateLink,\n",
    "    PredicateMention,\n",
    ")\n",
    "from forte.data.ontology import Annotation\n",
    "from forte.data.readers import OntonotesReader\n",
    "from forte.data.data_pack import DataPack\n",
    "from forte.pipeline import Pipeline\n",
    "data_path = os.path.abspath(\n",
    "            os.path.join(\"../data_samples\", \"ontonotes/one_file\"\n",
    "            )\n",
    "        )\n",
    "pipeline: Pipeline = Pipeline()\n",
    "pipeline.set_reader(OntonotesReader())\n",
    "pipeline.initialize()\n",
    "data_pack: DataPack = pipeline.process_one(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then we can run the following code to get the full document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :   The Indonesian billionaire James Riady has agreed to pay $ 8.5 million and plead guilty to illegally donating money for Bill Clinton 's 1992 presidential campaign . He admits he was trying to influence American policy on China .\n"
     ]
    }
   ],
   "source": [
    "for doc_idx, d in enumerate(data_pack.get_data(context_type=Document)):\n",
    "    print(doc_idx, \":  \", d['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the generator generates a dictionary each iteration (in this dummy case we only have one iteration) and the document data is retrieved by dictionary key `'context'`.\n",
    "\n",
    "To better understand this, let's consider a more concrete case. Since the document containing two sentences, suppose we want to retrieve text data sentence by sentence for a linguistic analysis task. In other words, we expect two dictionaries in the generator and each dictionary stores a sentence.\n",
    "\n",
    "We can get each sentence by the following code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The Indonesian billionaire James Riady has agreed to pay $ 8.5 million and plead guilty to illegally donating money for Bill Clinton 's 1992 presidential campaign .\n",
      "1 He admits he was trying to influence American policy on China .\n"
     ]
    }
   ],
   "source": [
    "data_generator = data_pack.get_data(context_type=Sentence)\n",
    "for sent_idx, d in enumerate(data_generator):\n",
    "    print(sent_idx, d['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get the two sentences by two iterations.\n",
    "\n",
    "So far, we introduce two examples to explain the first parameter `context_type` which controls the granularity of the data context. Depending on the task, we can generate data of different granularities. We assigned `context_type` from `Document` to `Sentence` for sentence tasks, and we can even further change it to `Token` for token tasks.\n",
    "\n",
    "Suppose we don't want to analyze the first sentence in the `data_pack`, there is `skip_k` parameter that skips k data of `context_type` and starts generating data from (k+1)th instance. In this case, we want to start generating from the second instance so we set `skip_k` to 1 to skip the first instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 He admits he was trying to influence American policy on China .\n"
     ]
    }
   ],
   "source": [
    "data_generator = data_pack.get_data(context_type=Sentence, skip_k=1)\n",
    "for sent_idx, d in enumerate(data_generator):\n",
    "    print(sent_idx, d['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* more advanced data retrieval\n",
    "* we want to do part-of-speech tagging for each sentence\n",
    "* d['Token'] contains four keys 'span' 'text' 'pos' 'tid', four lists with the same length (=num of tokens)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The Indonesian billionaire James Riady has agreed to pay $ 8.5 million and plead guilty to illegally donating money for Bill Clinton 's 1992 presidential campaign .\n",
      "['DT' 'JJ' 'NN' 'NNP' 'NNP' 'VBZ' 'VBN' 'TO' 'VB' '$' 'CD' 'CD' 'CC' 'VB'\n",
      " 'JJ' 'IN' 'RB' 'VBG' 'NN' 'IN' 'NNP' 'NNP' 'POS' 'CD' 'JJ' 'NN' '.']\n",
      "1 He admits he was trying to influence American policy on China .\n",
      "['PRP' 'VBZ' 'PRP' 'VBD' 'VBG' 'TO' 'VB' 'JJ' 'NN' 'IN' 'NNP' '.']\n",
      "dict_keys(['span', 'text', 'pos', 'tid'])\n"
     ]
    }
   ],
   "source": [
    "requests = {\n",
    "    Token: [\"pos\"],\n",
    "}\n",
    "data_generator = data_pack.get_data(context_type=Sentence, request=requests)\n",
    "for sent_idx, d in enumerate(data_generator):\n",
    "    print(sent_idx, d['context'])\n",
    "    print(d['Token']['pos'])\n",
    "print(d['Token'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* So far we have introduced three \"data types\", `Document`, `Sentence`, and \n",
    " User can request particular data fields within the range of a particular `Annotation` or `AudioAnnotation` type. User request particular data fields by setting `request` and the search range by setting `context_type`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Annotation\n",
    "In forte, each annotation has a range which includes begin and end of annotation-specific data of that particular annotation. For `Annotation` type, range means the begin index and end index of characters under `Annotation` type in the `text` payload of the `DataPack`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The\n",
      "1 The Indonesian billionaire James Riady\n",
      "2 The Indonesian billionaire James Riady\n",
      "3 The Indonesian billionaire James Riady\n",
      "4 The Indonesian billionaire James Riady\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'The Parent entry of ft.onto.base_ontology.PredicateLink is not requested. You should also request PredicateMention with ft.onto.base_ontology.PredicateLink'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d82944120861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[0mdata_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_pack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAnnotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mann_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/murphy/forte/forte/data/data_pack.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, context_type, request, skip_k)\u001b[0m\n\u001b[1;32m   1014\u001b[0m                         )\n\u001b[1;32m   1015\u001b[0m                     data[l_type.__name__] = self._generate_link_entry_data(\n\u001b[0;32m-> 1016\u001b[0;31m                         \u001b[0ml_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m                     )\n\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# TODO: Getting Group based on range is not done yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/murphy/forte/forte/data/data_pack.py\u001b[0m in \u001b[0;36m_generate_link_entry_data\u001b[0;34m(self, a_type, a_args, data, cont)\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparent_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 1175\u001b[0;31m                     \u001b[0;34mf\"The Parent entry of {a_type} is not requested.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m                     \u001b[0;34mf\" You should also request {parent_type} with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                     \u001b[0;34mf\"{a_type}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'The Parent entry of ft.onto.base_ontology.PredicateLink is not requested. You should also request PredicateMention with ft.onto.base_ontology.PredicateLink'"
     ]
    }
   ],
   "source": [
    "requests = {\n",
    "    Sentence: [\"speaker\"],\n",
    "    Token: [\"pos\", \"sense\"],\n",
    "    PredicateMention: [],\n",
    "    PredicateArgument: {\"fields\": [], \"unit\": \"Token\"},\n",
    "    PredicateLink: {\n",
    "        \"component\": utils.get_full_module_name(OntonotesReader),\n",
    "        \"fields\": [\"parent\", \"child\", \"arg_type\"],\n",
    "    },\n",
    "}\n",
    "data_generator = data_pack.get_data(Annotation, requests)\n",
    "for ann_idx, ann in enumerate(data_generator):\n",
    "    print(ann_idx, ann['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " For an `Token` instance which is a subtype of `Annotation`, its annotation-specific data is `text` and therefore range means the begin and end of characters of that `Token` instance. For an `Recording` instance which is a subtype of `AudioAnnotation`, its annotation-specific data is `audio` and there range means the begin and end index of that `Recording` instance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AudioAnnotation\n",
    "Based on the idea of \"range\", in the example code, entry `AudioUtterance` will be searched in `DataPack.audio_annotations` and the requested data field `speaker` will be included in the generator's data.\n",
    "\n",
    "For `AudioAnnotation` type, range means the begin index and end index of sound sample under `AudioAnnotation` type in the `audio` payload of the `DataPack`. \n",
    "\n",
    "For example, if User wants to get data of `AudioAnnotation` from a `DataPack` instance `pack`. User can call the function like the code blow. It returns a generator that User can iterate over.\n",
    "`AudioAnnotation` is passed into the method as parameter `context_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pack.get_data(AudioAnnotation,\n",
    "                {\n",
    "                AudioUtterance:\n",
    "                    {\"fields\": [\"speaker\"]}\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Coverage Index\n",
    "`DataPack.get()` is commonly used to retrieve entries from a datapack. In some cases, we are only interested in getting entries from a specific range. `DataPack.get()` allows users to set `range_annotation` which controls the search area of the sub-types. If `DataPack.get()` is called frequently with queries related to the `range_annotation`, you may consider building the coverage index regarding the related entry types. Users can call `DataPack.build_coverage_for(context_type, covered_type)` in order to create a mapping between a pair of entry types and target entries that are covered in ranges specified by outer entries.\n",
    "\n",
    "For example, if you need to get all the `Token`s from some `Sentence`, you can write your code as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the sentences in the pack.\n",
    "for sentence in input_pack.get(Sentence):\n",
    "    # Take all tokens from a sentence\n",
    "    token_entries = input_pack.get(\n",
    "        entry_type=Token, range_annotation=sentence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the snippet above may become a bottleneck if you have a lot of `Sentence` and `Token` entries inside the datapack. To speed up this process, you can build a coverage index first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build coverage index between `Token` and `Sentence`\n",
    "input_pack.build_coverage_for(\n",
    "    context_type=Sentence\n",
    "    covered_type=Token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `DataPack.build_coverage_for(context_type, covered_type)` function is able to build a mapping from `context_type` to `covered_type`, allowing faster retrieval of inner entries covered by outer entries inside the datapack.\n",
    "We also provide a function called `DataPack.covers(context_entry, covered_entry)` for coverage checking. It returns `True` if the span of `covered_entry` is covered by the span of `context_entry`.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35c5ad93f26e4a012fe0ce2a15a836f7204b7396c27ea7588e034222fd2bde38"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('forte_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
