from abc import abstractmethod
from typing import (
    Dict, List, Iterator, Iterable, Union, Any, Optional, Tuple, Type)

import texar.torch as tx

from nlp.pipeline.data.dataset import Dataset
from nlp.pipeline.data.data_pack import DataPack
from nlp.pipeline.data.io_utils import merge_batches, batch_instances
from nlp.pipeline.data.ontology import Entry, Annotation

__all__ = [
    "Batcher",
    "ProcessingBatcher",
    "TexarBatcher"
]


class Batcher:
    """
    Base class of all batchers.

    Args:
        batch_size (int): the size of the batches this batcher will create.
    """
    def __init__(self, batch_size: int):
        self.batch_size = batch_size

    @abstractmethod
    def get_batch(self, *args) -> Iterator[Dict]:
        """
        Returns an iterator of data batches.
        """
        raise NotImplementedError


class DictData(tx.data.DataBase[Dict, Dict]):
    r"""A dataset that reads processed paired text from dumped NumPy files.

    Args:
        filename (str): The path to the dumped NumPy file.
        hparams: A `dict` or instance of :class:`~texar.HParams` containing
            hyperparameters. See :meth:`default_hparams` for the defaults.
        device: The device of the produces batches. For GPU training, set to
            current CUDA device.
    """

    def __init__(self, dataset: Iterator[Dict], hparams=None):

        data: Iterator[Dict] = dataset
        source = tx.data.IterDataSource(data)
        super().__init__(source, hparams)

    @staticmethod
    def default_hparams():
        return {
            **tx.data.DataBase.default_hparams(),
        }

    def process(self, raw_example: Dict) -> Dict:  # pylint: disable=no-self-use
        return raw_example

    def collate(self, examples: List[Dict]) -> tx.data.Batch:  # pylint: disable=no-self-use
        batch: Dict[str, Any] = {}
        for e in examples:
            for entry, fields in e.items():
                if isinstance(fields, dict):
                    if entry not in batch.keys():
                        batch[entry] = {}
                    for k, value in fields.items():
                        if k not in batch[entry].keys():
                            batch[entry][k] = []
                        batch[entry][k].append(value)
                else:  # context level feature
                    if entry not in batch.keys():
                        batch[entry] = []
                    batch[entry].append(fields)
        return tx.data.Batch(
            len(examples),
            **batch
        )


class TexarBatcher(Batcher):
    """
    A wrapper of the Texar Batching support. This batcher support batching
    techniques such as shuffling, caching, etc.

    Args:
        data_packs (list[DataPack] or iterator[DataPack]): either an iterator
            or a list of datapacks in the dataset.
        context_type (str): The granularity of the data context, which
            could be any annotation type.
        requests (dict, optional): The entry types and fields required.
            The keys of the requests dict are the required entry types
            and the value should be either:

            - a list of field names or
            - a dict which accepts three keys: `"fields"`, `"component"`,
              and `"unit"`.

                - By setting `"fields"` (list), users
                  specify the requested fields of the entry. If "fields"
                  is not specified, only the default fields will be
                  returned.
                - By setting `"component"` (list), users
                  can specify the components by which the entires are
                  generated. If `"component"` is not specified, will return
                  entries generated by all components.
                - By setting `"unit"` (string), users can
                  specify a unit by which the annotations are indexed.

            Note that for all annotation types, `"text"` and `"span"`
            fields are returned by default; for all link types, `"child"`
            and `"parent"` fields are returned by default.

        batch_size (int, optional): the size of the batches this batcher will
            create.
        hparams (optional): see `Texar <https://texar-pytorch.readthedocs.io/en/
            latest/code/data.html#database>`_ hparams for :class:`DataBase`.
    """
    def __init__(
            self,
            data_packs: Iterable[DataPack],
            context_type: Type[Annotation],
            requests: Optional[Dict[Type[Entry], Union[Dict, List]]] = None,
            batch_size: Optional[int] = None,
            hparams=None):

        if batch_size is not None:
            hparams["batch_size"] = batch_size
        dataset = Dataset(data_packs)
        data = DictData(dataset.get_data(context_type, requests),
                        hparams=hparams)
        super().__init__(data.batch_size)
        self.batch_iter = tx.data.DataIterator(data)

    def get_batch(self) -> Iterator[Dict]:   # type: ignore
        for batch in self.batch_iter:
            yield batch._batch


class ProcessingBatcher(Batcher):
    """
    A Batcher used in
    :class:`~nlp.pipeline.processors.batch_processor.BatchProcessor`.
    This Batcher only batches data sequentially.
    The Batcher receives new packs
    dynamically and cache the current packs so that the processors can
    pack prediction results into the data packs.

    Args:
        context_type (str): The granularity of the data context, which
            could be any annotation type.
        requests (dict): The entry types and fields required.
            The keys of the requests dict are the required entry types
            and the value should be either:

            - a list of field names or
            - a dict which accepts three keys: `"fields"`, `"component"`,
              and `"unit"`.

                - By setting `"fields"` (list), users
                  specify the requested fields of the entry. If "fields"
                  is not specified, only the default fields will be
                  returned.
                - By setting `"component"` (list), users
                  can specify the components by which the entires are
                  generated. If `"component"` is not specified, will return
                  entries generated by all components.
                - By setting `"unit"` (string), users can
                  specify a unit by which the annotations are indexed.

            Note that for all annotation types, `"text"` and `"span"`
            fields are returned by default; for all link types, `"child"`
            and `"parent"` fields are returned by default.
        batch_size (int): the size of the batches this batcher will create.
        hard_batch (bool, optional): whether to allow batches smaller than
            ``batch_size`` or not. If `True`, will generate smaller batches
            for the tail instances in each pack. Otherwise, only generate at
            most one small batch for the tail instances in the last pack.
    """

    def __init__(
            self,
            batch_size: int,
            context_type: Type[Annotation],
            requests: Optional[Dict[Type[Entry], Union[Dict, List]]] = None,
            hard_batch: bool = False):
        super().__init__(batch_size)

        self.hard_batch = hard_batch

        self.current_batch: Dict = {}
        self.instance_num_in_current_batch = 0

        self.data_pack_pool: List[DataPack] = []
        self.current_batch_sources: List[int] = []

        self.context_type: Type[Annotation] = context_type
        self.requests: Optional[Dict[Type[Entry], Union[Dict, List]]] = requests

    def get_batch(  # type: ignore
            self,
            input_pack: Optional[DataPack],
            tail_instances: bool = False) -> Iterator[Dict]:
        """
        Returns an iterator of data batches.
        """
        if input_pack is None:  # No more packs, return the tail instances
            if self.current_batch:
                yield self.current_batch
                self.current_batch = {}
                self.instance_num_in_current_batch = 0
                self.current_batch_sources = []
        else:  # cache the new pack and generate batches
            self.data_pack_pool.append(input_pack)
            for (data_batch, instance_num) in self._get_data_batch_by_need(
                    input_pack, self.context_type, self.requests):

                self.current_batch = merge_batches(
                    [self.current_batch, data_batch])

                self.instance_num_in_current_batch += instance_num
                self.current_batch_sources.append(instance_num)

                if (tail_instances or not self.hard_batch or
                        self.instance_num_in_current_batch == self.batch_size):
                    yield self.current_batch

                    self.current_batch = {}
                    self.instance_num_in_current_batch = 0
                    self.current_batch_sources = []

    def _get_data_batch_by_need(
            self,
            data_pack: DataPack,
            context_type: Type[Annotation],
            requests: Optional[Dict[Type[Entry], Union[Dict, List]]] = None,
            offset: int = 0) -> Iterable[Tuple[Dict, int]]:
        """
        Try to get batches of size ``batch_size``. If the tail instances cannot
        make up a full batch, will generate a small batch with the tail
        instances.

        Returns:
            An iterator of tuples ``(batch, cnt)``, ``batch`` is a dict
            containing the required annotations and context, and ``cnt`` is
            the number of instances in the batch.
        """

        instances: List[Dict] = []
        for data in data_pack.get_data(context_type, requests, offset):

            instances.append(data)
            if (len(instances) ==
                    self.batch_size - self.instance_num_in_current_batch):
                batch = batch_instances(instances)
                yield (batch, len(instances))
                instances = []

        if len(instances):
            batch = batch_instances(instances)
            yield (batch, len(instances))
